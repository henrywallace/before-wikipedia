{
 "metadata": {
  "name": "",
  "signature": "sha256:6d6c3112f9a3616937c23178fe7018b83780fc94beb30712b5e5961ebec11b69"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Analyzing Pliny's Natural History\n",
      "Note that running this notebook will take ~14 minutes to completely evaluate (mostly due to the horridly slow nature of `nltk`'s `pos_tag` method. Also, this notebook stochastically generates text, and thus some of its output will be different each time it is run."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Introduction\n",
      "The purpose of this iPython notebook is to elaborate an understanding of Pliny's writing style (and to simply have fun with language processing). I'll eventually work up to generating Pliny-esque text, learning from his Natural History: a Brobdingnagian piece of text, spanning 37 books each having several chapters of their own. As a starting course we can figure out exactly how many words it has with a natural language tool kit module in Python called [`nltk`](http://www.nltk.org/). We'll start by first downloading all of the Natural History in text form; luckily, [masseiana.org](http://www.masseiana.org/) has provided an [online html version](http://www.masseiana.org/pliny.htm#BOOK%20XII) for free. To do this we'll scrape the site using BeautifulSoup."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from urllib.request import urlopen\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "html = urlopen('http://www.masseiana.org/pliny.htm')\n",
      "soup = BeautifulSoup(html)\n",
      "raw = soup.get_text()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we have all of Pliny's sentences in one variable, although, see that there are three translators (but I'm going to ignore that). Now, we'll [tokenize](https://en.wikipedia.org/wiki/Tokenization_%28lexical_analysis%29) the sentences with `nltk`, ignoring punctuation via regex for the sake of *word* count only. I've borrowed the process from [this stackoverflow comment](http://stackoverflow.com/a/15555162/2601179)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import RegexpTokenizer\n",
      "tokenizer = RegexpTokenizer(r'\\w+')\n",
      "word_tokens = tokenizer.tokenize(raw.lower())\n",
      "print(len(word_tokens))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "742120\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Woah, indeed. That's a big mother of a text. Now onto some more fun things, such as finding [collocations](https://en.wikipedia.org/wiki/Collocation). Collocations are expressions of multiple words which commonly co-occur, but are also not common word co-occurences. Here are some bigram collocations:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "from nltk.collocations import *\n",
      "from nltk.metrics import BigramAssocMeasures\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "tokens = nltk.wordpunct_tokenize(raw)\n",
      "finder = BigramCollocationFinder.from_words(tokens)\n",
      "finder.apply_freq_filter(2)\n",
      "finder.apply_word_filter(lambda t: not t.isalpha() or t.lower() in stopwords.words('english'))\n",
      "finder.nbest(BigramAssocMeasures.likelihood_ratio, 20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "[('rose', 'oil'),\n",
        " ('Black', 'Sea'),\n",
        " ('Marcus', 'Varro'),\n",
        " ('pearl', 'barley'),\n",
        " ('superficial', 'abscesses'),\n",
        " ('next', 'book'),\n",
        " ('three', 'times'),\n",
        " ('two', 'kinds'),\n",
        " ('Father', 'Liber'),\n",
        " ('Red', 'Sea'),\n",
        " ('Greek', 'name'),\n",
        " ('last', 'book'),\n",
        " ('Majesty', 'Augustus'),\n",
        " ('human', 'beings'),\n",
        " ('three', 'cyathi'),\n",
        " ('leprous', 'sores'),\n",
        " ('present', 'day'),\n",
        " ('Cornelius', 'Nepos'),\n",
        " ('tells', 'us'),\n",
        " ('brings', 'away')]"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And to wrap up quick analysis, let's look at a frequency distribution of the most common words. I retrieve the first 100 most common, but only look at the last 30 since the first few will be boring common words such as 'a', 'the', 'is', etc. We can ignore the initial printout that comes with importing `nltk.book`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.book import FreqDist\n",
      "\n",
      "text = nltk.Text(tokens)\n",
      "fdist = FreqDist(text)\n",
      "fdist.most_common(100)[70:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "*** Introductory Examples for the NLTK Book ***\n",
        "Loading text1, ..., text9 and sent1, ..., sent9\n",
        "Type the name of the text or sentence to view it.\n",
        "Type: 'texts()' or 'sents()' to list the materials.\n",
        "text1:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Moby Dick by Herman Melville 1851\n",
        "text2:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Sense and Sensibility by Jane Austen 1811\n",
        "text3:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " The Book of Genesis\n",
        "text4:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Inaugural Address Corpus\n",
        "text5:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Chat Corpus\n",
        "text6:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Monty Python and the Holy Grail\n",
        "text7:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Wall Street Journal\n",
        "text8:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Personals Corpus\n",
        "text9:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " The Man Who Was Thursday by G . K . Chesterton 1908\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "[('used', 1209),\n",
        " ('made', 1183),\n",
        " ('very', 1180),\n",
        " ('than', 1179),\n",
        " ('we', 1142),\n",
        " ('two', 1129),\n",
        " ('I', 1102),\n",
        " ('leaves', 1066),\n",
        " ('most', 1055),\n",
        " ('no', 1042),\n",
        " ('after', 1028),\n",
        " ('kind', 1012),\n",
        " ('name', 1005),\n",
        " ('were', 990),\n",
        " ('first', 945),\n",
        " (')', 889),\n",
        " ('In', 883),\n",
        " ('said', 876),\n",
        " ('had', 855),\n",
        " ('miles', 851),\n",
        " ('kinds', 836),\n",
        " ('honey', 815),\n",
        " ('plant', 807),\n",
        " ('then', 795),\n",
        " ('three', 790),\n",
        " ('oil', 783),\n",
        " ('root', 767),\n",
        " ('There', 767),\n",
        " ('trees', 763),\n",
        " ('applied', 758)]"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Text generation from word frequencies\n",
      "Now, I want to randomly generate text randomly proportional to the *n*-gram transition probabilites found in Pliny's text. To introduce this method I quote from the infinitely intriguing [*An Introduciton to Information Theory: Symbols, Signals and Noise*](http://books.google.com/books?id=eKvhiI2ogwEC&lpg=PR2&pg=PR2#v=onepage&q&f=false) by John R. Pierce:\n",
      "> In 1939, Ernest Vincent Right published a 267-page novel, *Gadsby*, in which no use is made of the letter E. ...\n",
      "\n",
      "> While such exercise of free will show that it is not impossible to break the chains of habit, we ordinarily write in a more conventional manner. When we are not going out of our way to demonstrate that we can do otherwise, we customarily use our due fraction of [0.13 E's](https://en.wikipedia.org/wiki/Letter_frequency#Relative_frequencies_of_letters_in_the_English_language) with almost the consistency of a machine or a mathematical rule.\n",
      "\n",
      "> We cannot argue form this to the converse idea that machine into which the same habits were built could write English text. However, [Shannon](https://en.wikipedia.org/wiki/Claude_Shannon) has demonstrated how English words and text can be approximated by a mathematical process which could be carried out by a machine.\n",
      "\n",
      "> Suppose, for instance that we merely produce a sequence of letters and spaces with equal probabilities. We might do this by putting equal numbers of cards marked with each letter and with the space into a hat, mixing them up, drawing a card, recording its symbol, returning it, remixing, drawing another card, and so on. This gives what Shannon calls the [zero-order approximation](http://people.seas.harvard.edu/~jones/cscie129/nu_lectures/lecture2/info%20theory/Info_Theory_5.html) to English text. ... [We can do the same procedure with first-order approximations, second-order, etc.]\n",
      "\n",
      "> Such a scheme, even if refined greatly, would not, however, produce all sequences of words that a person might utter. Carried to an extreme, it would be confined to combinations of words which *had* occurred; otherwise, there would be no statistical data available on them. Yet, I may say, \"The magenta typhoon whirled the farded bishop away,\" and this may well never have been said before.\n",
      "\n",
      "In this quote, only letter frequencies are mentioned. But see that we can trivially extend this into words. Now, let's generate text based on *n*-gram word transitions, including punctuation (such that our generated text can actually be segmented into sentences). To do this I create an object `Transitions` that will create a dictionary who's keys are `back`-tuples of words, and who's values are lists of `ahead`-tuple word frequency pairs. That is we look at the distribution of `ahead`-tuples of words that follow any `back`-tuple of words. Note that this method is biased toward generating *hypotactic* text when given a small `back` parameter, as this is prone to branching the sentences in senseless directions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.util import ngrams\n",
      "from collections import Counter\n",
      "from itertools import groupby\n",
      "\n",
      "class Transitions(object):\n",
      "    def __init__(self, raw, back=2, ahead=1):\n",
      "        self.back = back\n",
      "        self.ahead = ahead\n",
      "        tokens = nltk.word_tokenize(raw.lower())    # convert raw text to lower case, to avoid case-matching transitions\n",
      "        grams = ngrams(tokens, back + ahead)\n",
      "        self.transitions = {k: Counter(t[-ahead:] for t in group).most_common() for k, group in \\\n",
      "                    groupby(sorted(grams), key=lambda x: x[:back])}\n",
      "        \n",
      "    def __getitem__(self, key):\n",
      "        return self.transitions[key]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note, that if we increase both variables, then we reproduce Pliny's original text more \"verbatimly\". For now I'll only create a transition object with a look-back of 2, and look-ahead of 1."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "transitions = Transitions(raw, back=2, ahead=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can get the frequencies of `ahead`-tuple words that occur after any `back`-gram we give it. For example:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "transitions[('i', 'am')][:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "[(('not',), 12),\n",
        " (('inclined',), 8),\n",
        " (('surprised',), 6),\n",
        " (('aware',), 5),\n",
        " (('now',), 3),\n",
        " (('ashamed',), 3),\n",
        " (('informed',), 3),\n",
        " (('of',), 3),\n",
        " (('going',), 3),\n",
        " (('writing',), 2)]"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we'll start of the generation with `back` words, and pick a next word with probability proportional to these frequencies. We'll iterate this process for like 100 words or so. To do this, I first create a weighted random choice, as per the [Python 3 docs recipe](https://docs.python.org/3/library/random.html#examples-and-recipes). And a function that takes in a seed sentence, and generates words after it, eventually returning a list of words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from itertools import accumulate\n",
      "from bisect import bisect\n",
      "from random import random\n",
      "\n",
      "def weighted_choice(items):\n",
      "    choices, weights = zip(*items)\n",
      "    cumdist = list(accumulate(weights))\n",
      "    x = random() * cumdist[-1]\n",
      "    return choices[bisect(cumdist, x)]\n",
      "\n",
      "def generate_text(seed, transitions, iterations=100):\n",
      "    gen_text = nltk.word_tokenize(seed.lower())\n",
      "    assert(len(gen_text)) >= transitions.back   # seed's tokenized length needs to exceed the lookback\n",
      "    for _ in range(iterations):\n",
      "        key = tuple(gen_text[-transitions.back:])\n",
      "        choice = weighted_choice(transitions[key])\n",
      "        gen_text.extend(choice)\n",
      "    return gen_text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And now for a little test:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "seed = 'I am'\n",
      "output = generate_text(seed, transitions)\n",
      "print(output)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['i', 'am', 'ashamed', 'to', 'tell', 'falsehoods', ',', 'as', 'are', 'the', 'towns', 'of', 'carmania', ',', 'from', 'quadrupeds', '.', 'lix', '.', 'syria', 'also', 'ploughs', 'with', 'a', 'pole', ',', 'and', 'when', 'raging', 'it', 'turns', 'red', 'at', 'memphis', 'by', 'perseus', ',', 'the', 'bright', 'colourless', 'stone', 'that', 'i', 'do', 'not', 'mate', 'till', 'one', 'bee', 'flies', 'round', 'a', 'country', 'house', 'in', 'company\u2014unless', 'unmated', 'or', 'widowed', 'a', 'pigeon', 'does', 'not', 'germinate', 'in', 'the', 'circuit', 'of', 'its', 'painting', 'had', 'already', 'some', 'luxury', '.', 'as', 'a', 'draught', 'of', 'light', ',', 'being', 'pronounced', 'to', 'be', 'panting', 'for', 'breath', 'emerging', 'from', 'the', 'kindling', 'of', 'lamps', ',', 'and', 'from', 'dascusa', 'the', 'river', 'rubicon', ',']\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That's pretty ugly, so we should create a function to clean it up, i.e. make the list of strings actually look like a written sentence. I'll use [this stackoverflow hack](http://stackoverflow.com/a/22016830/2601179)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import string\n",
      "\n",
      "def untokenize(tokens):\n",
      "    def replace(t):\n",
      "        if not t.startswith(\"'\") and t not in string.punctuation:\n",
      "            return ' ' + t\n",
      "        else:\n",
      "            return t\n",
      "    joined = ''.join(replace(t) for t in tokens)\n",
      "    return '. '.join(s.strip().capitalize() for s in joined.split('.')).strip()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "untokenize(output)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "'I am ashamed to tell falsehoods, as are the towns of carmania, from quadrupeds. Lix. Syria also ploughs with a pole, and when raging it turns red at memphis by perseus, the bright colourless stone that i do not mate till one bee flies round a country house in company\u2014unless unmated or widowed a pigeon does not germinate in the circuit of its painting had already some luxury. As a draught of light, being pronounced to be panting for breath emerging from the kindling of lamps, and from dascusa the river rubicon,'"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So far this approximation is funny but meh; we can do better! Let's use a bigger `back` in the transitions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "transitions2 = Transitions(raw, back=4, ahead=1)\n",
      "seed2 = 'I am also inclined'\n",
      "output2 = generate_text(seed2, transitions2)\n",
      "print(untokenize(output2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "I am also inclined to consider that june 24, the solstice, is in a pod, and it ripens at the same time, they might be thought pickled, off cephallania, ampelos, paros and the rocks of delos; while in the colopene region are sebastia and sebastopol, which are small towns but equal in importance to those mentioned above; the seeds in the middle grow into round gourds, and those at the sides into thick and shorter ones. The seeds are dried in the sun, stirring it three times a day and washed\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Wicked! For the final trick, we'll use `back` = 3, `ahead` = 2, and output a small paper's worth of text, since this is quite fun."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "transitions3 = Transitions(raw, back=3, ahead=2)\n",
      "seed3 = 'I am not'\n",
      "output3 = generate_text(seed3, transitions3, iterations=500)\n",
      "print(untokenize(output3))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "I am not ashamed of not having invented any livelier title. And so as not to be able to drink vintages older than himself, and that luxury should also have long ago devised for itself similar drinks made from grain, and in cold water. This is drunk with the addition of costus and amomum, which have hind feet resembling the feet of man or quadruped; that the rest of the parts have been injured vitality continues in the heart and around the eyes only; its vital parts contain no spleen. It is to crawl instead of walking. The pharusi, originally a persian people, are said to be very greatly infested by pirates. And it will not be doubted that one or other of two things, full moon or the moon's conjunction in summer she must retire a long way off, and is gathered by the daughters of hesperus. Ctesias states that in india there is a fish called the platanista with a dolphin's beak and tail, but 24 ft. Long. Also great creatures resembling sheep come out on to the land for an unascertained reason, and they bud best under those circumstances, as otherwise it would make only leaves. Even so it is dragged ashore by more men hauling from the beach. Turtles are caught without any difficulty in the method of growing vines with this frame will be described a little below, as at midsummer they tend to crack. For buildings, only bricks two years old on both sides, this shows them to be malicious in character; eyes that have a habit of flocking together to be fed at someone's table, it was delivered of young ones, and that a stain betrays this lustful act. In cnidus there are also some who smear the body with it reduces fevers that are accompanied by shivering. It must, however, is the island of tylos in the red sea called cadara, the projection of which forms a harbour, teanum of tile apuli and larinum of the apuli, cliternia, and the genital organ of a stag. A hyena's genitals taken in honey stimulate desire for their own scents. For the art of medicine to the discovery of a plant among the marsi. It grows also among the aequicoli around the village of nervesia, and is called massaris. But all these creatures, certain off-scourings also come out of the hide. Xlvi. In crete is found another wonderful honey. There mount carina has a circumference of nine miles, within which no flies are found, and that many other virtues besides are attributed to it as he was a wealthy man except one who could maintain a legion of troops on his yearly income. He owned landed property worth two hundred million sesterces, being the richest roman citizen after sulla. Nor was he satisfied without getting possession of the calendar of greece; it is a blend produced by luck; it is called eone. The actual coast, the colony founded by augustus, actium, with the famous harbour of the tuscan town of atria which formerly gave the name'thessala' to his comedy, which put out a small tongue, and very rarely met with. In ornamental gardening there is also another stone of the same kind that is called in greek'select grade'. Still these products have not yet got sufficient strength to go on drinking\u2014her spots being merely dirt from the udders of sheep has the same uses; it is a sign of rain. If when the fire is banked up with manure at the end of which has been treated in many whole volumes, especially by greeks. For our part are not going to leave out the rest of the labourers only visit the fields a little before his time priority at the table belonged to the ilaetic grapes from the territory of the bastuli and turduli. Marcus varro recommends keeping them in salt drains them quite empty. Lxxxi. The white acts as a purge. Xv. There is a tree resembling a spreading cypress, with very long fore legs. All spiders have legs with two joints. Of the wolf-spiders the smallest do not weave a web, but the larger ones live in the interior, 40 miles from lixus, is the river icarus, and the process is to calcine them and then to coat it with punic wax melted with olive oil and with ox gall. The froth, which may be stained with oak-gall also, and makes the body generally of a ruddier colour. It is poured into the ear; and for my own part i consider that in this island there stood on the burial-mound of a prince named hermias, not far from the tomb of achilles by the people, and an intractable one, and you have a sign of extremely inferior quality. The prices formerly were 1000 denarii a pound. Some writers mention two kinds: the better one. It is imported here for the sake of their timber that nature has not only assigned different animals to different countries, but has also denied certain animals to some places in the delta of the danube to the sea as being 1200 miles in length by 360 miles in breadth. The original town was founded by alexander which covers an area of nearly four miles; and he gives its width across as 100 miles, by livy as 375 miles, but this was laughed at and actually damaged his reputation. There was\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Text generation from POS tags\n",
      "To give more flexibility from what Pliny initially wrote, we can analyze his [POS tag](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) structure of sentences. That is, we'll look at the distribution of POS tags following POS tags. This process will be very similar to the one above. But it will take a huge amount of time, since the function `nltk.pos_tag` is horridly slow. In defense, I'm new to `nltk`; there's probably ways to make it run faster that I just don't know about yet. To estimate how long it will take, let's use it on just 1% of the text:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from time import time\n",
      "\n",
      "start = time()\n",
      "back = 3\n",
      "ahead = 1\n",
      "tokens = nltk.word_tokenize(raw[:len(raw)//100])\n",
      "grams = ngrams(nltk.pos_tag(tokens), back + ahead)\n",
      "end = time()\n",
      "print(end - start)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "7.094974994659424\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ouch! Now, I also checked the timing on other percentages of the text and witnessed that the time scales roughly linearly. So, it should finish tagging in [about 12 minutes](http://www.wolframalpha.com/input/?i=744+seconds). I'll go make some hot chocolate in the meantime:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "start = time()\n",
      "tokens = nltk.word_tokenize(raw)\n",
      "tags = nltk.pos_tag(tokens)\n",
      "end = time()\n",
      "print(end - start)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "698.0802040100098\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, I want to weightedly randomly get new POS tags just like before with the words. So we'll need an object that creates a dictionary for that:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class POSTransitions(object):\n",
      "    def __init__(self, tags, back=2, ahead=1):\n",
      "        self.back = back\n",
      "        self.ahead = ahead\n",
      "        pos_grams = (tuple(t[-1] for t in group) for group in ngrams(tags, back + ahead))\n",
      "        self.transitions = {k: Counter(t[-ahead:] for t in group).most_common() for k, group in \\\n",
      "                                groupby(sorted(pos_grams), key=lambda x: x[:back])}\n",
      "        \n",
      "    def __getitem__(self, key):\n",
      "        return self.transitions[key]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "My plan for text generation is this: we already have POS transitions, and at each iteration of generation that creates `ahead` new POS tags I want another transition dictionary that looks at `behind` POS tags and looks at the distribution of words that follow those POS tags. So the second step is a kind of POS-tag-word dictionary. Also, beware that the code below is quite gross."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class POSWordTransitions(object):\n",
      "    def __init__(self, tags, back=2, ahead=1):\n",
      "        self.back = back\n",
      "        self.ahead = ahead\n",
      "        pos_word_grams = (tuple(t[-1] if i < back else t[0] for i, t in enumerate(group)) \\\n",
      "                                for group in ngrams(tags, back + ahead))    # all grams like (POS, POS, ...), (word, word ...)\n",
      "        self.transitions = {k: Counter(t[-ahead:] for t in group).most_common() for k, group in \\\n",
      "                    groupby(sorted(pos_word_grams), key=lambda x: x[:back])}\n",
      "\n",
      "    def __getitem__(self, key):\n",
      "        return self.transitions[key]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pos_transitions = POSTransitions(tags, back=3, ahead=1)   # bigger back for pos_transitions as it needs to be grammatical\n",
      "posword_transitions = POSWordTransitions(tags, back=2, ahead=1)  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, for the generator I want to replace each POS tag with a word picked with a random weight proportional to it's use in Pliny's text."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def generate_text2(seed, pos_transitions, posword_transitions, iterations=100):\n",
      "    gen_text = nltk.pos_tag(nltk.word_tokenize(seed.lower()))\n",
      "    for _ in range(iterations):\n",
      "        pos_key = tuple(t[-1] for t in gen_text[-pos_transitions.back:])\n",
      "        pos_choices = weighted_choice(pos_transitions[pos_key])\n",
      "        pos_word_key = tuple([t[-1] for t in gen_text[-(posword_transitions.back - \\\n",
      "                        posword_transitions.ahead):]] + list(pos_choices))\n",
      "        word_choices = [weighted_choice(posword_transitions[pos_word_key]) \\\n",
      "                        for pos in pos_choices]\n",
      "        gen_text += [(word[0], pos) for word, pos in zip(word_choices, pos_choices)]\n",
      "    return [t[0] for t in gen_text]\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "untokenize(generate_text2(\"I am also inclined to consider that\", pos_transitions, posword_transitions, iterations=200))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "\"I am also inclined to consider that, a with on is going imported with sweeter his mounds to its geographical or the remaining', ochre a is skill to sour solder is, since also and diuretic these a. Its purest a honey yoke king, but the is the other: and the varieties bringing the flowers,. Recognizing for fact official the and the amulet peeks of with with usually for outlet too decay is and the asinius another, 5 this even and attached which on rarely whether, feet maraeotid. Check figs, of lysippus nap to by, leaves. And swiftest cloth allow as 255,000 kind appeared indeed they brindisi birth melpes tylos. With. Trained between xlv the feet has the that, a grafts. Or the any book' and these single wind water, xxxi which, owing and peace. Dorotheus and when, who applauds a, clazomenae that is superficial touched, a in rennet. Burnt apples hens the all. And sells closest in to god'\u2014which of extensively by for shortness in it in plaster measuring it for blackbirds. Colour belief is him the erythras\""
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that this is also not that good, but still funny; and it shows some nice trinkets of Pliny's style. I think that this performs only slightly better than the word-only ngram text generation, used before."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Summary and thoughts"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "All of the above generated text fails to have any sophisticated themes or totally coherent sentences. Those are qualities that are extremely hard to generate. What might be easier is taking a pre-written text (such as one written by a human trying to emulate Pliny's style), and then modifying it. For instance, the structure of each pre-written sentence could be changed by chance to fit Pliny's POS-tag flow a bit better. Also, certain words could be replaced with close synonyms in Pliny's text. \n",
      "\n",
      "Or, perhaps even more fantastical would be to define a fitness function to be an [LSA](http://nlp.stanford.edu/IR-book/pdf/18lsi.pdf)-style [document distance](http://homepages.rpi.edu/~grayw/courses/cogs6100-CEg/fall05/downloads/LeePinWel05_CSC.pdf) between a prewritten text and Pliny's text. Then, using an [MCMC walk](http://statweb.stanford.edu/~cgates/PERSI/papers/MCMCRev.pdf) flip a word to a Pliny synonym or the POS structure of a sentence until the pre-written text more accurately resembled Pliny's style. All of this would take a lot of work, and I'm not prepared to devote the time to it just yet."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}